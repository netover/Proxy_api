# Global application settings
app_name: "LLM Proxy API"
app_version: "2.0.0"
debug: false
host: "127.0.0.1"
port: 8000
api_keys:
  - "your-api-key-here"
api_key_header: "X-API-Key"
cors_origins:
  - "*"
rate_limit_rpm: 1000
rate_limit_window: 60
request_timeout: 300
circuit_breaker_threshold: 5
circuit_breaker_timeout: 60
log_level: "INFO"
log_file: null
config_file: "config.yaml"

condensation:
  max_tokens_default: 512
  error_patterns:
    - "context_length_exceeded"
    - "maximum context length"
    - "token limit exceeded"
    - "input too long"
  adaptive_enabled: true
  adaptive_factor: 0.5
  cache_ttl: 300
  dynamic_reload: true
  parallel_providers: 1
  cache_size: 1000
  cache_persist: true
  cache_redis_url: null  # Redis URL for distributed caching (optional)
  truncation_threshold: 4000
  fallback_strategies:
    - "truncate"
    - "secondary_provider"
    - "delay"
  delay: 2.0

providers:
  - name: "openai_default"
    type: "openai"
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    models:
      - "gpt-4"
    enabled: false
    priority: 10
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    rate_limit: null
    max_connections: 10
    keepalive_timeout: 30
    custom_headers: {}
