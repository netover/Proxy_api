# ==============================================================================
# LLM Proxy API Configuration
# ==============================================================================
# This file contains the complete configuration for the LLM Proxy API.
# It is written in YAML format.

# ------------------------------------------------------------------------------
# Application Metadata
# ------------------------------------------------------------------------------
app:
  name: "LLM Proxy API"
  version: "2.0.0"
  environment: "production" # Can be 'development', 'staging', or 'production'

# ------------------------------------------------------------------------------
# Server Configuration
# ------------------------------------------------------------------------------
server:
  host: "0.0.0.0"  # Host to bind the server to. "0.0.0.0" makes it accessible externally.
  port: 8000       # Port to run the server on.
  debug: false     # Enable debug mode (provides more verbose error pages).
  reload: false    # Enable auto-reloading when code changes (for development).

# ------------------------------------------------------------------------------
# OpenTelemetry Configuration for Observability
# ------------------------------------------------------------------------------
telemetry:
  enabled: true
  service_name: "llm-proxy"
  service_version: "2.0.0"
  jaeger:
    enabled: true
    endpoint: "http://localhost:14268/api/traces"
  zipkin:
    enabled: true
    endpoint: "http://localhost:9411/api/v2/spans"
  sampling:
    probability: 1.0  # 1.0 = 100% sampling. Reduce in production (e.g., 0.1 for 10%).

# ------------------------------------------------------------------------------
# Jinja2 Template Configuration
# ------------------------------------------------------------------------------
templates:
  enabled: true
  directory: "templates"
  cache_size: 1000
  auto_reload: false

# ------------------------------------------------------------------------------
# Chaos Engineering (for resilience testing)
# ------------------------------------------------------------------------------
chaos_engineering:
  enabled: false  # WARNING: Enable only for testing purposes.
  faults:
    - type: "delay"
      probability: 0.1
      duration_ms: 500
    - type: "error"
      probability: 0.05
      error_code: 503
    - type: "timeout"
      probability: 0.08
      duration_ms: 5000

# ------------------------------------------------------------------------------
# Rate Limiting
# ------------------------------------------------------------------------------
rate_limit:
  requests_per_window: 1000
  window_seconds: 60
  burst_limit: 50
  # Per-route limits (e.g., "100/minute"). These override the global settings.
  routes:
    "/v1/chat/completions": "100/minute"
    "/v1/embeddings": "200/minute"

# ------------------------------------------------------------------------------
# Authentication
# ------------------------------------------------------------------------------
# It is recommended to use `api_keys_env` for production.
# The value should be the name of an environment variable that contains a
# comma-separated list of your API keys. Example: export PROXY_API_KEYS="key1,key2"
auth:
  api_keys_env: "PROXY_API_KEYS"
  # The `api_keys` list is still supported for local development or backward compatibility.
  api_keys:
    - "dev-key-for-local-testing"

# ------------------------------------------------------------------------------
# Provider Configuration
# ------------------------------------------------------------------------------
# Define the downstream AI providers the proxy will connect to.
providers:
  - name: "openai"
    type: "openai"
    api_key_env: "OPENAI_API_KEY" # Name of the env var holding the key.
    base_url: "https://api.openai.com/v1"
    models: ["gpt-3.5-turbo", "gpt-4"]
    enabled: true
    forced: false # If true, all requests go to this provider regardless of model name.
    priority: 1   # Lower numbers are higher priority for fallbacks.
    timeout: 30
    max_retries: 3
    max_connections: 50
    max_keepalive_connections: 20
    keepalive_expiry: 30.0
    retry_delay: 1.0
    custom_headers:
      "X-Client-Type": "llm-proxy"

  - name: "anthropic"
    type: "anthropic"
    api_key_env: "ANTHROPIC_API_KEY"
    base_url: "https://api.anthropic.com"
    models: ["claude-3-haiku", "claude-3-sonnet"]
    enabled: true
    forced: false
    priority: 2
    timeout: 30
    max_retries: 3
    max_connections: 50
    max_keepalive_connections: 20
    keepalive_expiry: 30.0
    retry_delay: 1.0
    custom_headers:
      "X-Client-Type": "llm-proxy"

# ------------------------------------------------------------------------------
# Circuit Breaker Configuration
# ------------------------------------------------------------------------------
circuit_breaker:
  failure_threshold: 5      # Number of failures before the circuit opens.
  recovery_timeout: 60      # Seconds to wait before moving to HALF-OPEN state.
  half_open_max_calls: 3    # Number of test requests to allow in HALF-OPEN state.
  expected_exception: "ProviderError" # Exceptions that count as failures.

# ------------------------------------------------------------------------------
# Context Condensation (for reducing long contexts)
# ------------------------------------------------------------------------------
condensation:
  enabled: true
  truncation_threshold: 8000 # Max context length before proactive truncation.
  summary_max_tokens: 512    # Max tokens for the generated summary.
  cache_size: 1000           # Number of summaries to keep in the LRU cache.
  cache_ttl: 3600            # Seconds to keep a summary in the cache.
  cache_persist: true
  cache_redis_url: "${REDIS_URL}" # Use env var for Redis URL.
  error_patterns:
    - "context length"
    - "maximum context"

# ------------------------------------------------------------------------------
# Smart Caching Configuration
# ------------------------------------------------------------------------------
caching:
  enabled: true
  response_cache:
    max_size_mb: 100
    ttl: 1800
    compression: true
  summary_cache:
    max_size_mb: 50
    ttl: 3600
    compression: true

# ------------------------------------------------------------------------------
# Memory Management
# ------------------------------------------------------------------------------
memory:
  max_usage_percent: 85      # Max memory usage before triggering warnings/actions.
  gc_threshold_percent: 80   # Memory usage % to trigger garbage collection.
  monitoring_interval: 30    # Seconds between memory checks.
  cache_cleanup_interval: 300 # Seconds between cache cleanup runs.

# ------------------------------------------------------------------------------
# Global HTTP Client Configuration
# ------------------------------------------------------------------------------
http_client:
  timeout: 30
  connect_timeout: 10
  read_timeout: 30
  pool_limits:
    max_connections: 100
    max_keepalive_connections: 30
    keepalive_timeout: 30

# ------------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------------
logging:
  level: "INFO"  # Default log level. Can be overridden by LOG_LEVEL env var.
  format: "json" # 'json' for production, 'console' for development.
  file: "logs/app.log"
  rotation:
    max_size_mb: 100
    max_files: 5
