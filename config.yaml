# LLM Proxy API Configuration
#
# This file supports environment variable substitution.
# You can use ${VAR_NAME} to insert the value of an environment variable.
# Example:
#   api_key: "${MY_API_KEY}"
#
# Updated with OpenTelemetry, Chaos Engineering, and Load Testing features

app:
  name: "LLM Proxy API"
  version: "2.0.0"
  environment: "production"

server:
  host: "0.0.0.0"
  port: 8000
  debug: false
  reload: false

# OpenTelemetry Configuration
telemetry:
  enabled: true
  service_name: "llm-proxy"
  service_version: "2.0.0"
  jaeger:
    enabled: true
    endpoint: "http://localhost:14268/api/traces"
  zipkin:
    enabled: true
    endpoint: "http://localhost:9411/api/v2/spans"
  sampling:
    probability: 1.0  # 100% sampling for development, reduce in production

# Jinja2 Template Configuration
templates:
  enabled: true
  directory: "templates"
  cache_size: 1000
  auto_reload: false

# Chaos Engineering Configuration
chaos_engineering:
  enabled: false  # Enable only for testing
  faults:
    - type: "delay"
      severity: "medium"
      probability: 0.1
      duration_ms: 500
    - type: "error"
      severity: "low"
      probability: 0.05
      error_code: 503
      error_message: "Service temporarily unavailable"
    - type: "rate_limit"
      severity: "low"
      probability: 0.03
    - type: "timeout"
      severity: "medium"
      probability: 0.08
      duration_ms: 5000
    - type: "network_failure"
      severity: "high"
      probability: 0.02

# Rate Limiting
rate_limit:
  requests_per_window: 1000
  window_seconds: 60
  burst_limit: 50
  routes:
    "/health": "1000/minute"
    "/v1/chat/completions": "100/minute"
    "/v1/completions": "100/minute"
    "/v1/embeddings": "200/minute"
    "/v1/images/generations": "50/minute"
    "/v1/health": "1000/minute"
    "/v1/metrics": "500/minute"
    "/v1/models": "200/minute"
    "/v1/status": "500/minute"
    "/v1/providers": "200/minute"
    "/v1/config/reload": "50/minute"
    "/v1/config/status": "100/minute"
    "/v1/config/invalidate-cache": "50/minute"

# Authentication
auth:
  api_keys:
    - "test-key-123"
    - "prod-key-456"
    - "admin-key-789"

# Provider Configuration
providers:
  - name: "openai"
    type: "openai"
    api_key_env: "OPENAI_API_KEY"
    base_url: "https://api.openai.com/v1"
    models:
      - "gpt-3.5-turbo"
      - "gpt-4"
      - "gpt-4-turbo"
    enabled: true
    forced: false
    priority: 1
    timeout: 30
    max_retries: 3
    max_connections: 50
    max_keepalive_connections: 20
    keepalive_expiry: 30.0
    retry_delay: 1.0
    custom_headers:
      "X-Client-Type": "llm-proxy"

  - name: "anthropic"
    type: "anthropic"
    api_key_env: "ANTHROPIC_API_KEY"
    base_url: "https://api.anthropic.com"
    models:
      - "claude-3-haiku"
      - "claude-3-sonnet"
      - "claude-3-opus"
    enabled: true
    forced: false
    priority: 2
    timeout: 30
    max_retries: 3
    max_connections: 50
    max_keepalive_connections: 20
    keepalive_expiry: 30.0
    retry_delay: 1.0
    custom_headers:
      "X-Client-Type": "llm-proxy"

  - name: "azure"
    type: "azure_openai"
    api_key_env: "AZURE_OPENAI_KEY"
    base_url: "https://api.openai.azure.com"
    models:
      - "gpt-35-turbo"
      - "gpt-4"
    enabled: true
    forced: false
    priority: 3
    timeout: 30
    max_retries: 3
    max_connections: 50
    max_keepalive_connections: 20
    keepalive_expiry: 30.0
    retry_delay: 1.0
    custom_headers:
      "X-Client-Type": "llm-proxy"

# Circuit Breaker Configuration
circuit_breaker:
  failure_threshold: 5
  recovery_timeout: 60
  half_open_max_calls: 3
  expected_exception: "ProviderError"

# Context Condensation
condensation:
  enabled: true
  truncation_threshold: 8000
  summary_max_tokens: 512
  cache_size: 1000
  cache_ttl: 3600
  cache_persist: true
  cache_redis_url: "${REDIS_URL}"
  error_patterns:
    - "context length"
    - "maximum context"
    - "too long"
    - "exceeds maximum"

# Smart Caching
caching:
  enabled: true
  response_cache:
    max_size_mb: 100
    ttl: 1800
    compression: true
  summary_cache:
    max_size_mb: 50
    ttl: 3600
    compression: true

# Memory Management
memory:
  max_usage_percent: 85
  gc_threshold_percent: 80
  monitoring_interval: 30
  cache_cleanup_interval: 300

# HTTP Client Configuration
http_client:
  timeout: 30
  connect_timeout: 10
  read_timeout: 30
  pool_limits:
    max_connections: 100
    max_keepalive_connections: 30
    keepalive_timeout: 30

# Logging
logging:
  level: "INFO"  # Default level, overridden by LOG_LEVEL env var
  format: "json"
  file: "logs/app.log"
  rotation:
    max_size_mb: 100
    max_files: 5
  # Environment-specific overrides:
  # Development: Set LOG_LEVEL=DEBUG for detailed logging
  # Production: Set LOG_LEVEL=WARNING for minimal logging
  # Default: INFO level

# Health Check Configuration
health_check:
  interval: 30

services:
  context_service_url: "http://localhost:8001"
  timeout: 5
  providers: true
  context_service: true
  memory: true
  cache: true

# Load Testing Configuration
load_testing:
  tiers:
    light:
      users: 30
      duration: "5m"
      ramp_up: "30s"
      expected_rps: 5
    medium:
      users: 100
      duration: "5m"
      ramp_up: "1m"
      expected_rps: 20
    heavy:
      users: 400
      duration: "15m"
      ramp_up: "5m"
      expected_rps: 80
    extreme:
      users: 1000
      duration: "20m"
      ramp_up: "10m"
      expected_rps: 200

# Network Simulation
network_simulation:
  profiles:
    fast:
      min_delay: 10
      max_delay: 50
      jitter: 0.1
    medium:
      min_delay: 100
      max_delay: 300
      jitter: 0.2
    slow:
      min_delay: 500
      max_delay: 2000
      jitter: 0.3
    unreliable:
      min_delay: 1000
      max_delay: 5000
      jitter: 0.5
