# LLM Proxy API Configuration
# Updated with OpenTelemetry, Chaos Engineering, and Load Testing features

app:
  name: "LLM Proxy API"
  version: "2.0.0"
  environment: "production"

server:
  host: "0.0.0.0"
  port: 8000
  debug: false
  reload: false

# OpenTelemetry Configuration
telemetry:
  enabled: true
  service_name: "llm-proxy"
  service_version: "2.0.0"
  jaeger:
    enabled: true
    endpoint: "http://localhost:14268/api/traces"
  zipkin:
    enabled: true
    endpoint: "http://localhost:9411/api/v2/spans"
  sampling:
    probability: 1.0  # 100% sampling for development, reduce in production

# Jinja2 Template Configuration
templates:
  enabled: true
  directory: "templates"
  cache_size: 1000
  auto_reload: false

# Chaos Engineering Configuration
chaos_engineering:
  enabled: false  # Enable only for testing
  faults:
    - type: "delay"
      severity: "medium"
      probability: 0.1
      duration_ms: 500
    - type: "error"
      severity: "low"
      probability: 0.05
      error_code: 503
      error_message: "Service temporarily unavailable"
    - type: "rate_limit"
      severity: "low"
      probability: 0.03
    - type: "timeout"
      severity: "medium"
      probability: 0.08
      duration_ms: 5000
    - type: "network_failure"
      severity: "high"
      probability: 0.02

# Rate Limiting
rate_limit:
  requests_per_window: 1000
  window_seconds: 60
  burst_limit: 50

# Authentication
auth:
  api_keys:
    - "test-key-123"
    - "prod-key-456"
    - "admin-key-789"

# Provider Configuration
providers:
  - name: "openai"
    type: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    models:
      - "gpt-3.5-turbo"
      - "gpt-4"
      - "gpt-4-turbo"
    enabled: true
    priority: 1
    timeout: 30
    max_retries: 3

  - name: "anthropic"
    type: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com"
    models:
      - "claude-3-haiku"
      - "claude-3-sonnet"
      - "claude-3-opus"
    enabled: true
    priority: 2
    timeout: 30
    max_retries: 3

  - name: "azure"
    type: "azure"
    api_key: "${AZURE_OPENAI_KEY}"
    endpoint: "${AZURE_OPENAI_ENDPOINT}"
    deployment_name: "${AZURE_DEPLOYMENT_NAME}"
    models:
      - "gpt-35-turbo"
      - "gpt-4"
    enabled: true
    priority: 3
    timeout: 30
    max_retries: 3

# Circuit Breaker Configuration
circuit_breaker:
  failure_threshold: 5
  recovery_timeout: 60
  half_open_max_calls: 3
  expected_exception: "ProviderError"

# Context Condensation
condensation:
  enabled: true
  truncation_threshold: 8000
  summary_max_tokens: 512
  cache_size: 1000
  cache_ttl: 3600
  cache_persist: true
  cache_redis_url: "${REDIS_URL}"
  error_patterns:
    - "context length"
    - "maximum context"
    - "too long"
    - "exceeds maximum"

# Smart Caching
caching:
  enabled: true
  response_cache:
    max_size_mb: 100
    ttl: 1800
    compression: true
  summary_cache:
    max_size_mb: 50
    ttl: 3600
    compression: true

# Memory Management
memory:
  max_usage_percent: 85
  gc_threshold_percent: 80
  monitoring_interval: 30
  cache_cleanup_interval: 300

# HTTP Client Configuration
http_client:
  timeout: 30
  connect_timeout: 10
  read_timeout: 30
  pool_limits:
    max_connections: 100
    max_keepalive_connections: 30
    keepalive_timeout: 30

# Logging
logging:
  level: "INFO"
  format: "json"
  file: "logs/app.log"
  rotation:
    max_size_mb: 100
    max_files: 5

# Health Check Configuration
health_check:
  interval: 30
  timeout: 5
  providers: true
  context_service: true
  memory: true
  cache: true

# Load Testing Configuration
load_testing:
  tiers:
    light:
      users: 30
      duration: "5m"
      ramp_up: "30s"
      expected_rps: 5
    medium:
      users: 100
      duration: "5m"
      ramp_up: "1m"
      expected_rps: 20
    heavy:
      users: 400
      duration: "15m"
      ramp_up: "5m"
      expected_rps: 80
    extreme:
      users: 1000
      duration: "20m"
      ramp_up: "10m"
      expected_rps: 200

# Network Simulation
network_simulation:
  profiles:
    fast:
      min_delay: 10
      max_delay: 50
      jitter: 0.1
    medium:
      min_delay: 100
      max_delay: 300
      jitter: 0.2
    slow:
      min_delay: 500
      max_delay: 2000
      jitter: 0.3
    unreliable:
      min_delay: 1000
      max_delay: 5000
      jitter: 0.5
