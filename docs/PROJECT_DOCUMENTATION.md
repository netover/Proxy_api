# üìö Documenta√ß√£o Completa - LLM Proxy API

## Sum√°rio

- [Vis√£o Geral](#visao-geral)
- [Arquitetura do Sistema](#arquitetura-do-sistema)
- [Instala√ß√£o e Configura√ß√£o](#instalacao-e-configuracao)
- [Guia de Uso](#guia-de-uso)
- [APIs e Endpoints](#apis-e-endpoints)
- [Arquivos de Configura√ß√£o](#arquivos-de-configuracao)
- [Monitoramento e M√©tricas](#monitoramento-e-metricas)
- [Estrutura de Diret√≥rios](#estrutura-de-diretorios)
- [Desenvolvimento](#desenvolvimento)
- [Troubleshooting](#troubleshooting)
- [Refer√™ncias](#referencias)

---

## üéØ Vis√£o Geral

O **LLM Proxy API** √© uma solu√ß√£o enterprise-ready para proxy de APIs de Language Learning Models (LLMs), oferecendo:

### ‚ú® Principais Caracter√≠sticas

- **üîÑ Roteamento Inteligente**: Failover autom√°tico entre provedores baseado em sa√∫de e prioridade
- **üè• Monitoramento de Sa√∫de**: Sistema avan√ßado de health checks com cache e circuit breakers
- **üìä M√©tricas Abrangentes**: Monitoramento completo com Prometheus e m√©tricas customizadas
- **üîß Configura√ß√£o Flex√≠vel**: Suporte a m√∫ltiplas fontes de configura√ß√£o (YAML, JSON, env vars)
- **üõ°Ô∏è Seguran√ßa**: Autentica√ß√£o, rate limiting e valida√ß√£o de entrada
- **üìà Escalabilidade**: Connection pooling, concorr√™ncia controlada e cache inteligente
- **üîç Observabilidade**: Logging estruturado, tracing e dashboards
- **üöÄ Auto-Discovery**: Descoberta autom√°tica de provedores via registry/K8s/Consul

### üéØ Casos de Uso

- **Proxy Unificado**: Interface √∫nica para m√∫ltiplos provedores de LLM
- **Load Balancing**: Distribui√ß√£o inteligente de carga entre provedores
- **Failover Autom√°tico**: Continuidade de servi√ßo com fallback inteligente
- **Monitoramento Centralizado**: Dashboard √∫nico para todos os provedores
- **Custo Otimiza√ß√£o**: Roteamento baseado em custo e performance

---

## üèóÔ∏è Arquitetura do Sistema

### Componentes Principais

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LLM Proxy API                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ  FastAPI    ‚îÇ  ‚îÇ  Config     ‚îÇ  ‚îÇ  Metrics    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  Server     ‚îÇ  ‚îÇ  Manager    ‚îÇ  ‚îÇ  Collector  ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ Provider    ‚îÇ  ‚îÇ Health      ‚îÇ  ‚îÇ Circuit     ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ Factory     ‚îÇ  ‚îÇ Monitor     ‚îÇ  ‚îÇ Breaker     ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ Request     ‚îÇ  ‚îÇ Rate        ‚îÇ  ‚îÇ Cache       ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ Router      ‚îÇ  ‚îÇ Limiter     ‚îÇ  ‚îÇ Manager     ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fluxo de Dados

1. **Recebimento**: Request chega via FastAPI
2. **Autentica√ß√£o**: Valida√ß√£o de API key e rate limiting
3. **Roteamento**: Sele√ß√£o do provedor baseado em sa√∫de/prioridade
4. **Processamento**: Chamada para provedor com retry/circuit breaker
5. **Cache**: Armazenamento de resultados para otimiza√ß√£o
6. **Monitoramento**: Coleta de m√©tricas e logging
7. **Resposta**: Retorno formatado para cliente

### Provedores Suportados

| Provedor | Status | Endpoint | Caracter√≠sticas |
|----------|--------|----------|-----------------|
| OpenAI | ‚úÖ | `/v1/chat/completions` | GPT-4, GPT-3.5, Embeddings |
| Anthropic | ‚úÖ | `/v1/messages` | Claude 3, Opus, Sonnet |
| Grok | ‚úÖ | `/v1/complete` | xAI Models |
| Perplexity | ‚úÖ | `/v1/ask` | Search-focused |
| Blackbox | ‚úÖ | `/v1/chat/completions` | Multi-modal |
| OpenRouter | ‚úÖ | `/v1/chat/completions` | Aggregator |
| Cohere | ‚úÖ | `/v1/generate` | Text Generation |
| Azure OpenAI | ‚úÖ | `/openai/deployments/*` | Enterprise |

---

## üì¶ Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos

- **Python**: 3.9+
- **Sistema Operacional**: Linux/Windows/macOS
- **Mem√≥ria**: 2GB+ RAM
- **Espa√ßo em Disco**: 500MB+

### Instala√ß√£o R√°pida

```bash

# 1. Clone o reposit√≥rio
git clone https://github.com/your-org/llm-proxy-api.git
cd llm-proxy-api

# 2. Crie ambiente virtual
python -m venv venv

source venv/bin/activate  # Linux/macOS

# ou
venv\Scripts\activate     # Windows

# 3. Instale depend√™ncias
pip install -r requirements.txt

# 4. Configure vari√°veis de ambiente

cp .env.example .env
# Edite .env com suas chaves de API

# 5. Execute o servidor
python main_dynamic.py
```

### Configura√ß√£o Avan√ßada

#### Docker Deployment

```yaml
# docker-compose.yml
version: '3.8'
services:
  llm-proxy:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./config.yaml:/app/config.yaml
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
    restart: unless-stopped
```

#### Kubernetes

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-proxy-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-proxy-api
  template:
    metadata:
      labels:
        app: llm-proxy-api
    spec:
      containers:
      - name: llm-proxy
        image: llm-proxy-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"

```

### Verifica√ß√£o da Instala√ß√£o

```bash
# Teste b√°sico
curl http://localhost:8000/health

# Teste com provedor
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

---

## üöÄ Guia de Uso

### Uso B√°sico

```python
import requests

# Configura√ß√£o
API_KEY = "your-api-key"
BASE_URL = "http://localhost:8000"

# Chat Completion
response = requests.post(
    f"{BASE_URL}/v1/chat/completions",
    headers={
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    },
    json={
        "model": "gpt-4",
        "messages": [
            {"role": "user", "content": "Explain quantum computing"}
        ],
        "max_tokens": 500
    }
)

print(response.json())
```

### Streaming

```python
import requests

response = requests.post(
    f"{BASE_URL}/v1/chat/completions",
    headers={
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    },
    json={
        "model": "gpt-4",
        "messages": [{"role": "user", "content": "Tell me a story"}],
        "stream": True
    },
    stream=True
)

for line in response.iter_lines():
    if line:
        print(line.decode('utf-8'))
```

### JavaScript/TypeScript

```javascript
// Usando fetch
const response = await fetch('/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': 'Bearer your-api-key',
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'gpt-4',
    messages: [{role: 'user', content: 'Hello!'}],
    stream: true
  })
});

// Para streaming
const reader = response.body.getReader();
while (true) {
  const {done, value} = await reader.read();
  if (done) break;
  console.log(new TextDecoder().decode(value));
}
```

---

## üîå APIs e Endpoints

### Endpoints Principais

#### Chat Completions
```http
POST /v1/chat/completions
```

**Par√¢metros:**
- `model` (string): Nome do modelo
- `messages` (array): Array de mensagens
- `max_tokens` (int, opcional): M√°ximo de tokens
- `temperature` (float, opcional): Criatividade (0.0-2.0)
- `stream` (boolean, opcional): Streaming habilitado

**Exemplo:**
```json
{
  "model": "gpt-4",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Explain AI"}
  ],
  "max_tokens": 1000,
  "temperature": 0.7
}
```

#### Text Completions
```http
POST /v1/completions
```

#### Embeddings
```http
POST /v1/embeddings
```

### Endpoints de Monitoramento

#### Health Check
```http
GET /health
```

**Resposta:**
```json
{
  "status": "healthy",
  "timestamp": 1699999999.123,
  "providers": {
    "total": 8,
    "healthy": 7,
    "degraded": 1,
    "unhealthy": 0
  },
  "uptime": "99.95%",
  "last_check": "2023-11-01T12:00:00Z"
}
```

#### M√©tricas
```http
GET /metrics
```

#### Status dos Provedores
```http
GET /providers
```

### Endpoints Administrativos

#### Health Worker
```http
GET /status          # Status detalhado de todos os provedores
GET /status/{name}   # Status de provedor espec√≠fico
POST /check          # Trigger manual de health check
POST /discover       # Trigger descoberta de provedores
POST /cache/clear    # Limpar cache de health checks
```

### Autentica√ß√£o

Todos os endpoints requerem autentica√ß√£o via header:

```
Authorization: Bearer your-api-key
```

Ou via header customizado:

```
X-API-Key: your-api-key
```

---

## ‚öôÔ∏è Arquivos de Configura√ß√£o

### Estrutura de Configura√ß√£o

#### `config.yaml` - Configura√ß√£o Principal

```yaml
# Configura√ß√µes globais
app_name: "LLM Proxy API"
app_version: "2.0.0"
debug: false
host: "127.0.0.1"
port: 8000

# Autentica√ß√£o
api_keys:
  - "your-api-key-here"
api_key_header: "X-API-Key"

# Seguran√ßa
cors_origins:
  - "*"

# Rate Limiting
rate_limit_rpm: 1000
rate_limit_window: 60

# Timeouts
request_timeout: 300

# Circuit Breaker
circuit_breaker_threshold: 5
circuit_breaker_timeout: 60

# Logging
log_level: "INFO"
log_file: null

# Health Monitoring
health_check_interval: 60
health_check_cache_ttl: 30

# Provider Discovery
provider_discovery_interval: 300
provider_registry_url: null

# Condensation (Context Management)
condensation:
  max_tokens_default: 512
  error_keywords:
    - "context_length_exceeded"
    - "maximum context length"
  adaptive_factor: 0.5
  cache_ttl: 300

# Provedores
providers:
  - name: "openai_default"
    type: "openai"
    base_url: "https://api.openai.com/v1"
    api_key_env: "PROXY_API_OPENAI_API_KEY"
    models:
      - "gpt-4"
      - "gpt-3.5-turbo"
    enabled: true
    priority: 1
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
    max_connections: 10
    keepalive_timeout: 30
```

#### `health_worker_providers.json` - Configura√ß√£o de Health Checks

```json
{
  "providers": [
    {
      "name": "openai",
      "base_url": "https://api.openai.com",
      "models": ["gpt-3.5-turbo", "gpt-4"]
    },
    {
      "name": "anthropic",
      "base_url": "https://api.anthropic.com",
      "models": ["claude-3-sonnet"]
    }

  ]

}
```

#### `.env` - Vari√°veis de Ambiente

```bash
# API Keys
PROXY_API_OPENAI_API_KEY=sk-your-openai-key
PROXY_API_ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
PROXY_API_GROK_API_KEY=xai-your-grok-key

# Server Configuration
PROXY_API_PORT=8000
PROXY_API_HOST=127.0.0.1
LOG_LEVEL=INFO

# Health Worker
HEALTH_CHECK_INTERVAL=60
HEALTH_CHECK_CACHE_TTL=30
PROVIDER_DISCOVERY_INTERVAL=300

# HTTP Client
HTTP_MAX_CONNECTIONS=100
HTTP_MAX_KEEPALIVE=20
HTTP_TIMEOUT=5.0
```

### Valida√ß√£o de Configura√ß√£o

O sistema valida automaticamente todas as configura√ß√µes na inicializa√ß√£o:

```bash
python validate_env.py
```

---

## üìä Monitoramento e M√©tricas

### M√©tricas Prometheus

#### Counters
- `llm_proxy_requests_total{provider, endpoint, status}` - Total de requests
- `llm_proxy_errors_total{provider, error_type}` - Total de erros
- `llm_proxy_tokens_total{provider, operation}` - Total de tokens

#### Histograms
- `llm_proxy_request_duration_seconds{provider, endpoint}` - Dura√ß√£o de requests
- `llm_proxy_response_time_seconds{provider}` - Tempo de resposta

#### Gauges
- `llm_proxy_active_connections{provider}` - Conex√µes ativas
- `llm_proxy_circuit_breaker_state{provider}` - Estado do circuit breaker
- `llm_proxy_health_check_cache_size` - Tamanho do cache

### Health Checks

#### Provedor Health
```json
{
  "name": "openai",
  "status": "healthy",
  "last_check": 1699999999.123,
  "response_time": 0.245,
  "models": ["gpt-4", "gpt-3.5-turbo"],
  "circuit_breaker": {
    "state": "closed",
    "failure_count": 0,
    "last_failure_time": null
  }
}
```

#### Sistema Health
```json
{
  "status": "healthy",
  "uptime": "99.95%",
  "version": "2.0.0",
  "components": {
    "api_server": "healthy",
    "health_worker": "healthy",
    "metrics_collector": "healthy",
    "config_manager": "healthy"
  }
}
```

### Logging

#### Estrutura de Log

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "logger": "llm_proxy.core.request_router",
  "request_id": "req_123456",
  "message": "Request completed successfully",
  "provider": "openai",
  "model": "gpt-4",
  "tokens": 150,
  "duration": 0.245,
  "status_code": 200
}
```

#### N√≠veis de Log

- `DEBUG`: Detalhes t√©cnicos e debugging
- `INFO`: Opera√ß√µes normais e m√©tricas
- `WARNING`: Problemas n√£o cr√≠ticos
- `ERROR`: Erros que afetam funcionalidade
- `CRITICAL`: Erros cr√≠ticos do sistema

### Dashboards

#### Grafana Dashboard

```json
{
  "dashboard": {
    "title": "LLM Proxy API",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(llm_proxy_requests_total[5m])",
            "legendFormat": "{{provider}}"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(llm_proxy_errors_total[5m])",
            "legendFormat": "{{provider}} - {{error_type}}"
          }
        ]
      }
    ]
  }
}
```

---

## üìÅ Estrutura de Diret√≥rios

```
llm-proxy-api/
‚îú‚îÄ‚îÄ üìÑ main.py                 # Ponto de entrada principal
‚îú‚îÄ‚îÄ üìÑ main_dynamic.py         # Vers√£o din√¢mica com hot-reload
‚îú‚îÄ‚îÄ üìÑ web_ui.py              # Interface web de configura√ß√£o
‚îú‚îÄ‚îÄ üìÑ health_worker/         # Servi√ßo de monitoramento de sa√∫de
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ app.py
‚îú‚îÄ‚îÄ üìÑ requirements.txt       # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ config.yaml           # Configura√ß√£o principal
‚îú‚îÄ‚îÄ üìÑ .env                  # Vari√°veis de ambiente
‚îú‚îÄ‚îÄ üìÑ pytest.ini            # Configura√ß√£o de testes
‚îú‚îÄ‚îÄ üìÑ build_config.yaml     # Configura√ß√£o de build
‚îú‚îÄ‚îÄ üìÅ src/                  # C√≥digo fonte
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ api/              # Endpoints da API
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/             # Componentes core
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config.py     # Gerenciamento de configura√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ provider_factory.py  # Factory de provedores
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ request_router.py    # Roteamento de requests
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ providers/        # Implementa√ß√µes de provedores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ openai.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ anthropic.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ ...
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ services/         # Servi√ßos utilit√°rios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ logging.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ scripts/          # Scripts utilit√°rios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ export_dataset.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ validate_env.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/            # Utilit√°rios
‚îú‚îÄ‚îÄ üìÅ tests/                # Testes
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_providers.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_endpoints.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test_config.py
‚îú‚îÄ‚îÄ üìÅ docs/                 # Documenta√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README.md
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ export_dataset.md
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ PROJECT_DOCUMENTATION.md
‚îú‚îÄ‚îÄ üìÅ static/               # Assets est√°ticos
‚îú‚îÄ‚îÄ üìÅ templates/            # Templates HTML
‚îú‚îÄ‚îÄ üìÅ logs/                 # Logs do sistema
‚îú‚îÄ‚îÄ üìÅ exports/              # Dados exportados
‚îú‚îÄ‚îÄ üìÅ systemd-services/     # Servi√ßos systemd
‚îú‚îÄ‚îÄ üìÅ venv_test/            # Ambiente virtual de teste
‚îî‚îÄ‚îÄ üìÅ p/                    # Projetos relacionados
```

### Arquivos Importantes

#### Core Files
- `main_dynamic.py`: Ponto de entrada principal com hot-reload
- `config.yaml`: Configura√ß√£o central do sistema
- `requirements.txt`: Depend√™ncias Python

#### Configuration Files
- `.env`: Vari√°veis de ambiente sens√≠veis
- `health_worker_providers.json`: Configura√ß√£o de health checks
- `build_config.yaml`: Configura√ß√£o de build execut√°veis

#### Documentation
- `README.md`: Documenta√ß√£o principal
- `docs/export_dataset.md`: Guia de exporta√ß√£o de dados
- `docs/PROJECT_DOCUMENTATION.md`: Esta documenta√ß√£o completa

#### Services
- `health_worker/app.py`: Servi√ßo de monitoramento de sa√∫de
- `web_ui.py`: Interface web de administra√ß√£o
- `src/scripts/export_dataset.py`: Utilit√°rio de exporta√ß√£o

---

## üíª Desenvolvimento

### Configura√ß√£o do Ambiente de Desenvolvimento

```bash
# 1. Clone e setup
git clone https://github.com/your-org/llm-proxy-api.git
cd llm-proxy-api
python -m venv venv
source venv/bin/activate

# 2. Instale depend√™ncias de desenvolvimento
pip install -r requirements.txt
pip install -r requirements-dev.txt

# 3. Configure pre-commit hooks
pre-commit install

# 4. Execute testes
pytest tests/ -v

# 5. Execute linting
black src/

isort src/

flake8 src/
```

### Estrutura de C√≥digo

#### Padr√µes de Codifica√ß√£o

```python
# Exemplo de implementa√ß√£o de provedor
from src.core.provider_factory import BaseProvider
from src.core.unified_config import ProviderConfig

class MyProvider(BaseProvider):
    def __init__(self, config: ProviderConfig):
        super().__init__(config)

    async def _perform_health_check(self) -> Dict[str, Any]:
        """Implementa√ß√£o de health check"""
        async with self.http_client.get("/health") as response:
            return {
                "status": "healthy" if response.status == 200 else "unhealthy",
                "response_time": response.elapsed.total_seconds() * 1000
            }

    async def create_completion(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Implementa√ß√£o de chat completion"""
        # L√≥gica espec√≠fica do provedor
        pass
```

#### Testes

```python
# Exemplo de teste
import pytest
from unittest.mock import AsyncMock, patch

class TestMyProvider:
    @pytest.fixture
    def provider(self):
        config = ProviderConfig(
            name="test_provider",
            base_url="https://api.example.com",
            api_key="test-key"
        )
        return MyProvider(config)

    @pytest.mark.asyncio
    async def test_create_completion_success(self, provider):
        # Arrange
        request = {
            "model": "test-model",
            "messages": [{"role": "user", "content": "Hello"}]
        }

        # Act
        with patch.object(provider.http_client, 'post') as mock_post:
            mock_post.return_value = AsyncMock(
                status=200,
                json=AsyncMock(return_value={"choices": []})
            )
            result = await provider.create_completion(request)

        # Assert
        assert result is not None
```

### Adicionando Novos Provedores

1. **Criar classe do provedor** em `src/providers/`

2. **Implementar m√©todos abstratos** da `BaseProvider`
3. **Registrar no ProviderFactory**

4. **Adicionar configura√ß√£o** em `config.yaml`
5. **Escrever testes** abrangentes
6. **Atualizar documenta√ß√£o**

### Debugging

#### Logs Detalhados

```bash

# Execute com debug
LOG_LEVEL=DEBUG python main_dynamic.py

# Ou configure em config.yaml
log_level: "DEBUG"
log_file: "logs/debug.log"
```

#### Health Check Debug

```bash
# Verificar status de provedores
curl http://localhost:8000/providers

# Executar health check manual
curl -X POST http://localhost:8000/check

# Ver m√©tricas detalhadas
curl http://localhost:8000/metrics
```

---

## üîß Troubleshooting

### Problemas Comuns

#### 1. Erro de Autentica√ß√£o

**Sintomas:**
```
HTTP 401: Invalid API key
```

**Solu√ß√µes:**
```bash
# Verificar se API key est√° configurada
echo $PROXY_API_OPENAI_API_KEY

# Verificar formato do header
curl -H "Authorization: Bearer your-key" http://localhost:8000/v1/models

# Verificar configura√ß√£o
cat config.yaml | grep api_keys
```

#### 2. Provedor Indispon√≠vel

**Sintomas:**
```
HTTP 503: All providers are currently unavailable
```

**Solu√ß√µes:**
```bash
# Verificar status dos provedores
curl http://localhost:8000/health

# Executar health check manual

curl -X POST http://localhost:8000/check

# Verificar circuit breaker
curl http://localhost:8000/providers
```

#### 3. Rate Limiting

**Sintomas:**
```
HTTP 429: Too Many Requests
```

**Solu√ß√µes:**
```yaml

# Aumentar limites em config.yaml
rate_limit_rpm: 2000  # Aumentar RPM
rate_limit_window: 120  # Aumentar janela
```

#### 4. Timeout de Request

**Sintomas:**
```
HTTP 504: Gateway Timeout
```

**Solu√ß√µes:**

```yaml
# Aumentar timeouts em config.yaml
request_timeout: 600  # 10 minutos
circuit_breaker_timeout: 120  # 2 minutos
```

#### 5. Problemas de Mem√≥ria

**Sintomas:**

```
MemoryError ou alta utiliza√ß√£o de RAM
```

**Solu√ß√µes:**
```yaml
# Otimizar configura√ß√µes
max_connections: 50  # Reduzir conex√µes
health_check_cache_ttl: 15  # Cache menor
```

### Diagn√≥stico Avan√ßado

#### Verificar Logs

```bash
# Logs em tempo real
tail -f logs/proxy_api.log

# Filtrar por n√≠vel
grep "ERROR" logs/proxy_api.log

# Filtrar por provedor
grep "openai" logs/proxy_api.log

```

#### Analisar M√©tricas

```bash
# Request rate por provedor
curl http://localhost:8000/metrics | grep llm_proxy_requests_total

# Lat√™ncia m√©dia
curl http://localhost:8000/metrics | grep llm_proxy_request_duration

# Taxa de erro

curl http://localhost:8000/metrics | grep llm_proxy_errors_total
```

#### Debug de Conex√µes

```bash
# Verificar conex√µes ativas
netstat -tlnp | grep :8000

# Verificar processos
ps aux | grep python

# Verificar uso de recursos
top -p $(pgrep python)
```

### Recupera√ß√£o de Desastres

#### Backup de Configura√ß√£o

```bash
# Backup autom√°tico
cp config.yaml config.yaml.backup
cp .env .env.backup

# Restaura√ß√£o
cp config.yaml.backup config.yaml
cp .env.backup .env
```

#### Reset de Estado

```bash
# Limpar cache de health checks
curl -X POST http://localhost:8000/cache/clear

# Reset circuit breakers
# Reiniciar o servi√ßo
systemctl restart llm-proxy-api
```

---

## üìö Refer√™ncias

### Documenta√ß√£o Relacionada

- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Anthropic API Documentation](https://docs.anthropic.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Prometheus Metrics](https://prometheus.io/docs/practices/naming/)

#recursos-externos-llm-comparison-https-llm-comparison-com)
- [API Rate Limiting Best Practices](https://stripe.com/docs/rate-limits)
- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)

#ferramentas-de-desenvolvimento-poetry-https-python-poetry-org) - Gerenciamento de depend√™ncias
- [Black](https://black.readthedocs.io/) - Formata√ß√£o de c√≥digo
- [Pylint](https://pylint.readthedocs.io/) - Linting
- [pytest](https://docs.pytest.org/) - Testes

### Suporte

- **GitHub Issues**: Para bugs e feature requests
- **Discussions**: Para perguntas gerais
- **Wiki**: Tutoriais e guias avan√ßados

---

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a **MIT License**. Veja o arquivo `LICENSE` para detalhes.

---

*√öltima atualiza√ß√£o: Janeiro 2024*
*Vers√£o: 2.0.0*
